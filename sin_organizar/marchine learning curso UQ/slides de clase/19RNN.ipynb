{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Redes neuronales recurrentes (RNN, LSTM, GRU)\n",
        "\n",
        "Adaptado por: Jonnatan Arias Garcia\n",
        "\n",
        "jariasg@uniquindio.edu.co\n",
        "\n",
        "Jonnatan.arias@utp.edu.co\n",
        "\n",
        "Machine Learning - 2024\n",
        "Uniquindio - UTP"
      ],
      "metadata": {
        "id": "lddGM_tov3vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "oec4BQNlv8W-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# shakespeare (generador de secuencias de caracteres por shakespeare)"
      ],
      "metadata": {
        "id": "kWsVZqr0wD_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enlace = 'https://rawcdn.githack.com/moona740/Nat_Moore_MA_Thesis/ffd9d46fbc034042e26eae25d65f0e98f9418b6c/pf_1.txt'"
      ],
      "metadata": {
        "id": "XAzu9K7kwjfp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/sample_data/pf_1.txt https://rawcdn.githack.com/moona740/Nat_Moore_MA_Thesis/ffd9d46fbc034042e26eae25d65f0e98f9418b6c/pf_1.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm-aLOO09YiO",
        "outputId": "061e2e38-212c-4421-81d0-16ae99f82fe3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-21 01:21:55--  https://rawcdn.githack.com/moona740/Nat_Moore_MA_Thesis/ffd9d46fbc034042e26eae25d65f0e98f9418b6c/pf_1.txt\n",
            "Resolving rawcdn.githack.com (rawcdn.githack.com)... 104.21.234.231, 104.21.234.230, 2606:4700:3038::6815:eae7, ...\n",
            "Connecting to rawcdn.githack.com (rawcdn.githack.com)|104.21.234.231|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://raw.githubusercontent.com/moona740/Nat_Moore_MA_Thesis/ffd9d46fbc034042e26eae25d65f0e98f9418b6c/pf_1.txt [following]\n",
            "--2024-04-21 01:21:55--  https://raw.githubusercontent.com/moona740/Nat_Moore_MA_Thesis/ffd9d46fbc034042e26eae25d65f0e98f9418b6c/pf_1.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1149443 (1.1M) [text/plain]\n",
            "Saving to: ‘/content/sample_data/pf_1.txt’\n",
            "\n",
            "/content/sample_dat 100%[===================>]   1.10M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-04-21 01:21:56 (20.7 MB/s) - ‘/content/sample_data/pf_1.txt’ saved [1149443/1149443]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data inspección"
      ],
      "metadata": {
        "id": "UiJFs7ILJaVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('/content/sample_data/pf_1.txt', 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2q8nn4t-VKi",
        "outputId": "95b24683-f47f-4a05-f7c5-8f69866692d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1140435 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-ZdOdLE-g-I",
        "outputId": "23e23c46-cba0-4d59-fa09-e98982219e76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿<|startoftext|>\r\n",
            "dog bone, stapler,\r\n",
            "cribbage board, garlic press\r\n",
            "because this window is loose—lacks\r\n",
            "suction, lacks grip.\r\n",
            "bungee cord, bootstrap,\r\n",
            "dog leash, leather belt\r\n",
            "because this window had sash cords.\r\n",
            "they frayed. they broke.\r\n",
            "feather dus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d3FQHDWxbXJ",
        "outputId": "75ae9bd6-b520-40b1-8341-61670b31df9b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "121 unique characters\n",
            "['\\n', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'E', 'I', 'J', 'M', 'R', 'S', 'T', 'V', 'W', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '\\xa0', '¢', '\\xad', '½', 'à', 'á', 'â', 'ä', 'å', 'ç', 'è', 'é', 'ê', 'ì', 'í', 'ï', 'ñ', 'ó', 'ö', 'ù', 'ú', 'ü', 'ā', 'đ', 'ę', 'ł', 'ū', 'ǫ', '́', 'ắ', 'ế', 'ố', 'ồ', '\\u200b', '–', '—', '‘', '’', '“', '”', '•', '…', '\\u2028', '\\u2029', '\\u2060', '\\ufeff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesamiento del texto"
      ],
      "metadata": {
        "id": "0Abm4bK0Jfhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorizar el texto\n",
        "\n",
        "Antes de entrenar, necesitamos la cadrena a su representacion numerica. creando dos mapas, uno mapea caracteres a numeros y otra numeros a caracteres"
      ],
      "metadata": {
        "id": "8czcRs8bJjeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n"
      ],
      "metadata": {
        "id": "i8NkXCdiyL6S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mapeamos cada representacion a caracter, note que mapeamos con indices de 0 a len(unique)"
      ],
      "metadata": {
        "id": "gTwzpazuJ6C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3lYIhj4zwoq",
        "outputId": "ced58441-7d4e-47ed-bc70-24fedf43a578"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  '\\r':   1,\n",
            "  ' ' :   2,\n",
            "  '!' :   3,\n",
            "  '\"' :   4,\n",
            "  '#' :   5,\n",
            "  '$' :   6,\n",
            "  '%' :   7,\n",
            "  '&' :   8,\n",
            "  \"'\" :   9,\n",
            "  '(' :  10,\n",
            "  ')' :  11,\n",
            "  '*' :  12,\n",
            "  '+' :  13,\n",
            "  ',' :  14,\n",
            "  '-' :  15,\n",
            "  '.' :  16,\n",
            "  '/' :  17,\n",
            "  '0' :  18,\n",
            "  '1' :  19,\n",
            "  ...\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "#print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n",
        "print(text[1], text[2], text[3], text[4], text[5], text[6], text[7], text[8], text[9], text[10], text[11], text[12], text[13])\n",
        "print(text_as_int[1], text_as_int[2], text_as_int[3], text_as_int[4], text_as_int[5], text_as_int[6], text_as_int[7], text_as_int[8], text_as_int[9], text_as_int[10], text_as_int[11], text_as_int[12], text_as_int[13])\n",
        "print(char2idx.get('t'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn9ShcTdzype",
        "outputId": "806d0598-045d-46c0-d0d2-95aec6e08548"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "< | s t a r t o f t e x t\n",
            "30 74 66 67 48 65 67 62 53 67 52 71 67\n",
            "67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tarea de predicción\n",
        "\n",
        "Dado un carácter, o una secuencia de caracteres,\n",
        "\n",
        "¿cuál es el siguiente carácter más probable? Esta es la tarea para la cual estamos entrenando al modelo.\n",
        "\n",
        "La entrada al modelo será una secuencia de caracteres, y entrenamos al modelo para predecir la salida, es decir, el siguiente carácter en cada paso de tiempo.\n",
        "\n",
        "Dado que las RNN mantienen un estado interno que depende de los elementos vistos previamente, ¿dado todos los caracteres computados hasta este momento, cuál es el siguiente carácter?"
      ],
      "metadata": {
        "id": "gpWo5f7qKN_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crear ejemplos de entrenamiento y objetivos.\n",
        "A continuación, divida el texto en secuencias de ejemplo.\n",
        "\n",
        "Cada secuencia de entrada contendrá seq_length caracteres del texto.\n",
        "\n",
        "Para cada secuencia de entrada, los objetivos correspondientes contienen la misma longitud de texto, excepto desplazado un carácter hacia la derecha.\n",
        "\n",
        "Por lo tanto, divida el texto en fragmentos de seq_length+1. Por ejemplo, digamos que seq_length es 4 y nuestro texto es \"Hello\". La secuencia de entrada sería \"Hell\" y la secuencia objetivo \"o\".\n",
        "\n",
        "Para hacer esto, primero use la función tf.data.Dataset.from_tensor_slices para convertir el vector de texto en un flujo de índices de caracteres."
      ],
      "metadata": {
        "id": "dTGI2YkKLWhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1) # // = floor division, rounds down to nearest whole number\n",
        "print(examples_per_epoch)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qodzze2Tz38t",
        "outputId": "ed0bf2b6-dd3d-4756-9713-26fffbb87340"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11291\n",
            "﻿\n",
            "<\n",
            "|\n",
            "s\n",
            "t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El método de lotes nos permite convertir fácilmente estos caracteres individuales en secuencias del tamaño deseado."
      ],
      "metadata": {
        "id": "RPSJQENeMCHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCCYMdmIz62s",
        "outputId": "7222d574-ba82-4ac5-efaa-292a204a4147"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'\\ufeff<|startoftext|>\\r\\ndog bone, stapler,\\r\\ncribbage board, garlic press\\r\\nbecause this window is loose—lack'\n",
            "'s\\r\\nsuction, lacks grip.\\r\\nbungee cord, bootstrap,\\r\\ndog leash, leather belt\\r\\nbecause this window had sa'\n",
            "\"sh cords.\\r\\nthey frayed. they broke.\\r\\nfeather duster, thatch of straw, empty\\r\\nbottle of elmer's glue\\r\\n\"\n",
            "'because this window is loud—its hinges clack\\r\\nopen, clack shut.\\r\\nstuffed bear, baby blanket,\\r\\nsingle '\n",
            "\"crib newel\\r\\nbecause this window is split. it's dividing\\r\\nin two.\\r\\nvelvet moss, sagebrush,\\r\\nwillow bra\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para cada secuencia, duplíquela y desplácela para formar el texto de entrada y de destino utilizando el método de mapeo para aplicar una función simple a cada lote:"
      ],
      "metadata": {
        "id": "pR9uYE6TMHNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "tf.print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk_2Xu4G0P5v",
        "outputId": "4a639cc1-5d1c-4236-b398-a8a8f2de7db1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imprimir los primeros ejemplos de valores de entrada y objetivo:"
      ],
      "metadata": {
        "id": "ozm-AqdEMXki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
        "\n",
        "print(dataset.take(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goajWZVgMYMy",
        "outputId": "b662b66c-849d-4212-8972-aca7e1dcb730"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:  '\\ufeff<|startoftext|>\\r\\ndog bone, stapler,\\r\\ncribbage board, garlic press\\r\\nbecause this window is loose—lac'\n",
            "Target data: '<|startoftext|>\\r\\ndog bone, stapler,\\r\\ncribbage board, garlic press\\r\\nbecause this window is loose—lack'\n",
            "<_TakeDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada índice de estos vectores se procesa como un paso de tiempo. Para la entrada en el paso de tiempo 0, el modelo recibe el índice de \"F\" e intenta predecir el índice de \"i\" como el siguiente carácter. En el siguiente paso de tiempo, hace lo mismo pero la RNN considera el contexto del paso anterior además del carácter de entrada actual."
      ],
      "metadata": {
        "id": "cHqcrN7-MfF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-132zpN6-De",
        "outputId": "76235eb1-1c5a-447e-c953-0aa2d3509721"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step    0\n",
            "  input: 120 ('\\ufeff')\n",
            "  expected output: 30 ('<')\n",
            "Step    1\n",
            "  input: 30 ('<')\n",
            "  expected output: 74 ('|')\n",
            "Step    2\n",
            "  input: 74 ('|')\n",
            "  expected output: 66 ('s')\n",
            "Step    3\n",
            "  input: 66 ('s')\n",
            "  expected output: 67 ('t')\n",
            "Step    4\n",
            "  input: 67 ('t')\n",
            "  expected output: 48 ('a')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crear lotes de entrenamiento\n",
        "\n",
        "Utilizamos tf.data para dividir el texto en secuencias manejables. Pero antes de alimentar estos datos al modelo, necesitamos mezclar los datos y empaquetarlos en lotes."
      ],
      "metadata": {
        "id": "qB7trD_pMtDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWCpcC6E02Vw",
        "outputId": "284c7df0-5d82-46af-f027-758e962155c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construir el modelo\n",
        "\n",
        "Utilice tf.keras.Sequential para definir el modelo. Para este ejemplo simple, se utilizan tres capas para definir nuestro modelo:\n",
        "* tf.keras.layers.Embedding: La capa de entrada. Una tabla de búsqueda entrenable que asignará los números de cada carácter a un vector con dimensiones de embedding_dim;\n",
        "* tf.keras.layers.GRU: Un tipo de RNN con unidades de tamaño rnn_units (también puede usar una capa LSTM aquí)\n",
        "* tf.keras.layers.Dense: La capa de salida, con vocab_size salidas."
      ],
      "metadata": {
        "id": "3JvRlMkGMyv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 2048"
      ],
      "metadata": {
        "id": "yF0wY5hT04iB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "2pLdaypd06ot"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nf3YINB508Sa",
        "outputId": "414e2c3e-d4b0-43d0-a1a8-ba8787cdd157"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           30976     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 2048)          18882560  \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 121)           247929    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19161465 (73.10 MB)\n",
            "Trainable params: 19161465 (73.10 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para cada carácter, el modelo busca la incrustación, ejecuta el GRU un paso de tiempo con la incrustación como entrada y aplica la capa densa para generar digitos que predicen la probabilidad logarítmica del siguiente carácter.\n"
      ],
      "metadata": {
        "id": "NCl_hF1yNS1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruebe el modelo\n",
        "Ahora ejecute el modelo para ver que se comporte como se espera.\n",
        "Primero verifique la forma de la salida:"
      ],
      "metadata": {
        "id": "lSKoJ0e9NxuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6RXdwDW1I-w",
        "outputId": "1884831f-4c47-4c11-ce6e-59f43ad2ae17"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 121) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el ejemplo anterior, la longitud de la secuencia de entrada es de 100, pero el modelo se puede ejecutar en entradas de cualquier longitud:"
      ],
      "metadata": {
        "id": "F849lXtCN1Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "987eVx3NN5Ts",
        "outputId": "705ae869-b649-4964-bb56-5d3dfeb2d2fb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           30976     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 2048)          18882560  \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 121)           247929    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19161465 (73.10 MB)\n",
            "Trainable params: 19161465 (73.10 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para obtener predicciones reales del modelo, necesitamos muestrear de la distribución de salida, para obtener los índices reales de los caracteres. Esta distribución está definida por los logits sobre el vocabulario de caracteres. Nota: es importante muestrear de esta distribución, ya que tomar el argmax de la distribución puede hacer que el modelo se quede fácilmente atrapado en un bucle. Pruébalo para el primer ejemplo en el lote:"
      ],
      "metadata": {
        "id": "lU75BkaxN72O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "metadata": {
        "id": "iz-P0xdK1Dbj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto nos da, en cada paso de tiempo, una predicción del índice del próximo carácter."
      ],
      "metadata": {
        "id": "ysHQLKRsN_eL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj1QdcKA1NIT",
        "outputId": "c02c8d6c-037a-48f6-f178-78e14f9574f5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 40, 104,   8,  98,  48,  35,  43,  61, 107,   8,  23,  71,  43,\n",
              "        28,  39,  53,   4,  35,  98,  34,  99,  65, 115,   3,  75,  63,\n",
              "        30,  85,  42, 119,  76,   3,  72,  31,  76,  96,  15,  25,  20,\n",
              "        96,  47,  70, 120, 110,  48,  69,  22,  51, 116,  13,  98,  82,\n",
              "        73,  53,  24,   1,  46,  67,   8,  50, 107,  96,  64,  34,  94,\n",
              "        45,  56, 112,  44,  32,  11,  17, 100,  44,  87,  38, 113,  12,\n",
              "        44,  26, 119,   3,  21,  48,  62,  99,  61,  37,  11,  32,  54,\n",
              "        89, 115,  19,   0,  77, 118, 109,  89,  81])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decodifica esto para ver el texto predicho por este modelo no entrenado."
      ],
      "metadata": {
        "id": "4QgMqaTtOD5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TTNyz4fOFz_",
        "outputId": "ccc42cab-358d-4208-bfd4-98878c5d3335"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: \n",
            " 'ouds go, nevertheless,\\r\\nin their direction.\\r\\n<|endoftext|>\\r\\n<|startoftext|>\\r\\ncapitán profundo, capit'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'Sắ&đaEWnồ&5xW:Rf\"EđAęr•!\\xa0p<èV\\u2060¢!y=¢ü-72ü`w\\ufeff—av4d…+đäzf6\\r_t&cồüqAù]i’[>)/ł[êM“*[8\\u2060!3aoęnJ)>gí•1\\n\\xad\\u2029–íâ'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrena el modelo\n",
        "\n",
        "\n",
        "En este punto, el problema se puede tratar como un problema de clasificación estándar. Dado el estado RNN anterior y la entrada en este paso de tiempo, predice la clase del próximo carácter."
      ],
      "metadata": {
        "id": "Im2Ure6pOG6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adjunte un optimizador y una función de pérdida.\n",
        "\n",
        "La función de pérdida estándar tf.keras.losses.sparse_categorical_crossentropy funciona en este caso porque se aplica a lo largo de la última dimensión de las predicciones. Dado que nuestro modelo devuelve logit, necesitamos establecer el indicador from_logits."
      ],
      "metadata": {
        "id": "ScRdGyrdOSTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B29XUxky1Vq_",
        "outputId": "ea229093-83af-4150-f391-f141a1e5260b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 121)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.7958283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure el procedimiento de entrenamiento utilizando el método tf.keras.Model.compile. Utilizaremos tf.keras.optimizers.Adam con argumentos predeterminados y la función de pérdida."
      ],
      "metadata": {
        "id": "6-4yEM9YOYPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "zj_n8OZ_1XNB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurar puntos de control\n",
        "\n",
        "Utilice un tf.keras.callbacks.ModelCheckpoint para asegurarse de que los puntos de control se guarden durante el entrenamiento:"
      ],
      "metadata": {
        "id": "7gTHbEFLObLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%ls # Directory where the checkpoints will be saved\n",
        "checkpoint_dir = '/sample_data/training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNhESiTm2l8T",
        "outputId": "8f486786-9d7d-4744-c893-1844c21b7da0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejecutar el entrenamiento\n",
        "\n",
        "Para mantener un tiempo de entrenamiento razonable, utilice 10 épocas para entrenar el modelo. En Colab, configure el tiempo de ejecución en GPU para un entrenamiento más rápido."
      ],
      "metadata": {
        "id": "eUuWFJtPO5Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=2, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkxJS6_s1a9Y",
        "outputId": "95cfc5e2-cb8b-4e9b-af8a-dbd47e9b9d0a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "176/176 [==============================] - 441s 2s/step - loss: 2.5975\n",
            "Epoch 2/2\n",
            "176/176 [==============================] - 442s 3s/step - loss: 1.9610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "get_3rd_layer_output = K.function([model.layers[0].input],\n",
        "                                  [model.layers[1].output])\n",
        "layer_output = get_3rd_layer_output\n",
        "print(layer_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m16DH3O71gST",
        "outputId": "49f92791-8290-410e-8c75-11a2d532434f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function function.<locals>.func at 0x7defc3d4d480>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generar texto"
      ],
      "metadata": {
        "id": "VptKyTdnO_Gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restaurar el último punto de control\n",
        "\n",
        "Para mantener este paso de predicción simple, use un tamaño de lote de 1. Debido a la forma en que se transmite el estado de la RNN de paso a paso, el modelo solo acepta un tamaño de lote fijo una vez construido. Para ejecutar el modelo con un batch_size diferente, necesitamos reconstruir el modelo y restaurar los pesos desde el punto de control."
      ],
      "metadata": {
        "id": "hJ2DwmqPPBil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yNJlfSF53iYm",
        "outputId": "8b843ba2-f64a-48c6-fbbb-ef1436830aa8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/sample_data/training_checkpoints/ckpt_2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "YyQechvp3kIJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exYBQwSh3lfx",
        "outputId": "0958555a-3e19-43d9-d40e-e2716d9f46fe"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (1, None, 256)            30976     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (1, None, 2048)           18882560  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (1, None, 121)            247929    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19161465 (73.10 MB)\n",
            "Trainable params: 19161465 (73.10 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bucle de predicción\n",
        "\n",
        "El siguiente bloque de código genera el texto:\n",
        "* Comienza eligiendo una cadena de inicio, inicializando el estado RNN y estableciendo el número de caracteres a generar.\n",
        "\n",
        "* Obtén la distribución de predicción del siguiente carácter usando la cadena de inicio y el estado RNN.\n",
        "\n",
        "* Luego, utiliza una distribución categórica para calcular el índice del carácter predicho. Utiliza este carácter predicho como nuestra siguiente entrada al modelo.\n",
        "\n",
        "* El estado RNN devuelto por el modelo se retroalimenta al modelo para que ahora tenga más contexto, en lugar de solo un carácter. Después de predecir el siguiente carácter, los estados RNN modificados se vuelven a retroalimentar al modelo, que es cómo aprende a medida que obtiene más contexto de los caracteres predichos anteriormente.\n",
        "\n",
        "Al observar el texto generado, verás que el modelo sabe cuándo capitalizar, hacer párrafos e imitar un vocabulario de escritura similar al de Shakespeare. Con el pequeño número de épocas de entrenamiento, aún no ha aprendido a formar frases coherentes."
      ],
      "metadata": {
        "id": "I6QcZ8cUPHzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 500.0\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 0.45\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(int(num_generate)):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "OFaHM1-43oid"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=\"This \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJg4Qyll3qJ9",
        "outputId": "f191532f-1945-47cd-c9b0-6bd5d1983059"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This where i lead for the word the was rome in the marr of a planted strains in the recting the minds\r\n",
            "the propers and beling it me be a stread of the crose in the livers\r\n",
            "that conds all dister bedain prosed\r\n",
            "strees and fire and street in a pitcenting what its fires and clades on the cares and for a sain, the cand\r\n",
            "with the caret and gring and cire of the strands,\r\n",
            "the carred of the pirlow the was and who wat i placks and mast\r\n",
            "ald and the crocked the bread of the sund sind like the latters\r\n",
            "her side\n"
          ]
        }
      ]
    }
  ]
}